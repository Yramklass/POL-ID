# File: configs/custom/deim_training

__include__: [
  './DEIM-main/configs/deim_dfine/deim_hgnetv2_s_coco.yml', # Inherits base model structure
  './DEIM-main/configs/base/deim.yml',
  './DEIM-main/configs/custom/deim_detection.yml'        # dataloader
]

output_dir: ./outputs # A new output directory

# --- Adjust Learning Rate for Fine-tuning ---
# A common practice is to use a smaller learning rate
optimizer:
  type: AdamW
  params: 
    - 
      params: '^(?=.*backbone)(?!.*bn).*$'
      lr: 0.00002 # Reduced by 10x from original 0.0002
    - 
      params: '^(?=.*(?:norm|bn)).*$'     # except bias
      weight_decay: 0.

  lr: 0.00004 # Reduced by 10x from original 0.0004
  betas: [0.9, 0.999]
  weight_decay: 0.0001


# --- Adjust Training Schedule for Fine-tuning ---
# Likely don't need as many epochs
epoches: 60 

## Our LR-Scheduler
flat_epoch: 29   # 4 + 50 / 2
no_aug_epoch: 6

## Our DataAug
train_dataloader: 
  dataset: 
    transforms:
      policy:
        epoch: [4, 29, 44] 

  collate_fn:
    mixup_epochs: [4, 29]
    stop_epoch: 44